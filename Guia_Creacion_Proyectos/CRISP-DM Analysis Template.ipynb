{"cells":[{"cell_type":"markdown","metadata":{"id":"_UV2Im7MidCf"},"source":["\n","![Screenshot-2016-04-20-11.58.54.png](attachment:Screenshot-2016-04-20-11.58.54.png)\n","\n","**Each phase of the process:**\n","1. [Business understanding](#Businessunderstanding)\n","    1. [Assess the Current Situation](#Assessthecurrentsituation)\n","        1. [Inventory of resources](#Inventory)\n","        2. [Requirements, assumptions and constraints](#Requirements)\n","        3. [Risks and contingencies](#Risks)\n","        4. [Terminology](#Terminology)\n","        5. [Costs and benefits](#CostBenefit)\n","    2. [What are the Desired Outputs](#Desiredoutputs)\n","    3. [What Questions Are We Trying to Answer?](#QA)\n","2. [Data Understanding](#Dataunderstanding)\n","    1. [Initial Data Report](#Datareport)\n","    2. [Describe Data](#Describedata)\n","    3. [Initial Data Exploration](#Exploredata) \n","    4. [Verify Data Quality](#Verifydataquality)\n","        1. [Missing Data](#MissingData) \n","        2. [Outliers](#Outliers) \n","    5. [Data Quality Report](#Dataqualityreport)\n","3. [Data Preparation](#Datapreparation)\n","    1. [Select Your Data](#Selectyourdata)\n","    2. [Cleanse the Data](#Cleansethedata)\n","        1. [Label Encoding](#labelEncoding)\n","        2. [Drop Unnecessary Columns](#DropCols)\n","        3. [Altering Datatypes](#AlteringDatatypes)\n","        4. [Dealing With Zeros](#DealingZeros)\n","    3. [Construct Required Data](#Constructrequireddata)\n","    4. [Integrate Data](#Integratedata)\n","4. [Exploratory Data Analysis](#EDA)\n","5. [Modelling](#Modelling)\n","    1. [Modelling Technique](#ModellingTechnique)\n","    2. [Modelling Assumptions](#ModellingAssumptions)\n","    3. [Build Model](#BuildModel)\n","    4. [Assess Model](#AssessModel)\n","6. [Evaluation](#Evaluation)\n","7. [Deployment](#Deployment)\n","\n","https://www.sv-europe.com/crisp-dm-methodology/\n"]},{"cell_type":"code","source":[""],"metadata":{"id":"lNWfJqw2kx1j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jt3Pywq1idCa"},"source":["# Applying the CRISP-DM Method to a Business Problem"]},{"cell_type":"markdown","source":["#Clase 1 🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁"],"metadata":{"id":"iPYlWPTfjyag"}},{"cell_type":"markdown","metadata":{"id":"zmit4v3uidCg"},"source":["# 1. Stage One - Determine Business Objectives and Assess the Situation  <a class=\"anchor\" id=\"Businessunderstanding\"></a>\n","The first stage of the CRISP-DM process is to understand what you want to accomplish from a business perspective. Your organisation may have competing objectives and constraints that must be properly balanced. The goal of this stage of the process is to uncover important factors that could influence the outcome of the project. Neglecting this step can mean that a great deal of effort is put into producing the right answers to the wrong questions."]},{"cell_type":"markdown","metadata":{"id":"SKFDX_ZNidCh"},"source":["## 1.1 Assess the Current Situation<a class=\"anchor\" id=\"Assessthecurrentsituation\"></a>"]},{"cell_type":"markdown","metadata":{"id":"pQNHmxr7idCh"},"source":["This involves more detailed fact-finding about all of the resources, constraints, assumptions and other factors that you'll need to consider when determining your data analysis goal and project plan. \n"]},{"cell_type":"markdown","metadata":{"id":"NANsHoNuidCi"},"source":["### 1.1.1. Inventory of resources <a class=\"anchor\" id=\"Inventory\"></a>\n","List the resources available to the project including:\n","- Personnel:\n","- Data: \n","- Computing resources: \n","- Software: \n"]},{"cell_type":"markdown","metadata":{"id":"BIlRKUTNidCj"},"source":["### 1.1.2. Requirements, assumptions and constraints - <a class=\"anchor\" id=\"Requirements\"></a> \n","- Requirements of the project including the schedule of completion\n","- Required comprehensibility and quality of results\n","- Data security concerns as well as any legal issues. \n","- Assumptions made by the project. These may be assumptions about the data that can be verified during data mining, but may also include non-verifiable assumptions about the business related to the project. It is particularly important to list the latter if they will affect the validity of the results. \n","- List the constraints on the project. \n"]},{"cell_type":"markdown","metadata":{"id":"MC2QVRKsidCk"},"source":["### 1.1.3.Risks and contingencies <a class=\"anchor\" id=\"Risks\"></a>\n","- List the risks or events that might delay the project or cause it to fail. \n","- what action will you take if these risks or events take place? "]},{"cell_type":"markdown","metadata":{"id":"_Xech6MRidCl"},"source":["### 1.1.4.Terminology <a class=\"anchor\" id=\"Terminology\"></a>\n","- A glossary of relevant business terminology\n","- A glossary of data mining terminology, illustrated with examples relevant to the business problem in question. \n"]},{"cell_type":"markdown","metadata":{"id":"Oi6VmM6vidCl"},"source":["### 1.1.5.Costs and benefits  <a class=\"anchor\" id=\"CostBenefit\"></a>\n","- Construct a cost-benefit analysis for the project which compares the costs of the project with the potential benefits to the business if it is successful. This comparison should be as specific as possible. For example, you should use financial measures in a commercial situation. "]},{"cell_type":"markdown","metadata":{"id":"IKTPRdCRidCm"},"source":[" ## 1.2 What are the desired outputs of the project? <a class=\"anchor\" id=\"Desiredoutputs\"></a>"]},{"cell_type":"markdown","metadata":{"id":"8WH4taLsidCn"},"source":["\n","**Business success criteria**\n","- \n","- \n","\n","\n","\n","**Data mining success criteria**\n","- \n","- \n","\n","\n","**Produce project plan**\n","- \n","- \n","\n"]},{"cell_type":"markdown","metadata":{"id":"e95txqU_idCn"},"source":[" ## 1.3 What Questions Are We Trying To Answer? <a class=\"anchor\" id=\"QA\"></a>"]},{"cell_type":"markdown","metadata":{"id":"nRj8qKoGidCn"},"source":["- \n","- "]},{"cell_type":"markdown","source":["#Clase 2 🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁"],"metadata":{"id":"5fF4tMa0juWL"}},{"cell_type":"markdown","metadata":{"id":"nTY-FYgxidCo"},"source":["# 2. Stage  Two - Data Understanding <a class=\"anchor\" id=\"Dataunderstanding\"></a>\n","The second stage of the CRISP-DM process requires you to acquire the data listed in the project resources. This initial collection includes data loading, if this is necessary for data understanding. For example, if you use a specific tool for data understanding, it makes perfect sense to load your data into this tool. If you acquire multiple data sources then you need to consider how and when you're going to integrate these."]},{"cell_type":"markdown","metadata":{"id":"Z3Rs3fllidCo"},"source":["## 2.1 Initial Data Report <a class=\"anchor\" id=\"Datareport\"></a>\n","Initial data collection report - \n","List the data sources acquired together with their locations, the methods used to acquire them and any problems encountered. Record problems you encountered and any resolutions achieved. This will help both with future replication of this project and with the execution of similar future projects."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"cm-gp3kCidCp"},"outputs":[],"source":["# Import Libraries Required\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"gduRZLNfidCq","colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"status":"error","timestamp":1657159318968,"user_tz":300,"elapsed":735,"user":{"displayName":"Enrique Díaz","userId":"17549734348715053613"}},"outputId":"e353fc18-d9d8-46eb-c997-a2191f9631a6"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-77a78f065fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m'F:/Projects/Data Science/Defaults/train_/train.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# reads the data from the file - denotes as CSV, it has no header, sets column headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'F:/Projects/Data Science/Defaults/train_/train.csv'"]}],"source":["#Data source: \n","#Source Query location: \n","path =  'F:/Projects/Data Science/Defaults/train_/train.csv'\n","# reads the data from the file - denotes as CSV, it has no header, sets column headers\n","df =  pd.read_csv(path, sep=',') "]},{"cell_type":"markdown","metadata":{"id":"n7-MHOSbidCr"},"source":["## 2.2 Describe Data <a class=\"anchor\" id=\"Describedata\"></a>\n","Data description report - Describe the data that has been acquired including its format, its quantity (for example, the number of records and fields in each table), the identities of the fields and any other surface features which have been discovered. Evaluate whether the data acquired satisfies your requirements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRVRhpE5idCr","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"error","timestamp":1657159315531,"user_tz":300,"elapsed":17,"user":{"displayName":"Enrique Díaz","userId":"17549734348715053613"}},"outputId":"b201a9b3-0075-4723-e63e-f2e5f740746d"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b666bf274d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUQ8RBohidCr"},"outputs":[],"source":["df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Ot21fe9idCs"},"outputs":[],"source":["df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WgXQdAadidCs"},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Km-y0M8YidCs"},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLHJqcLoidCt"},"outputs":[],"source":["df.head(5)"]},{"cell_type":"markdown","metadata":{"id":"-YNj_1tGidCt"},"source":["## 2.3 Verify Data Quality <a class=\"anchor\" id=\"Verifydataquality\"></a>\n","\n","Examine the quality of the data, addressing questions such as:\n","\n","- Is the data complete (does it cover all the cases required)?\n","- Is it correct, or does it contain errors and, if there are errors, how common are they?\n","- Are there missing values in the data? If so, how are they represented, where do they occur, and how common are they?"]},{"cell_type":"markdown","metadata":{"id":"wzNzfXODidCt"},"source":["### 2.3.1. Missing Data <a class=\"anchor\" id=\"MissingData\"></a>\n","In addition to incorrect datatypes, another common problem when dealing with real-world data is missing values. These can arise for many reasons and have to be either filled in or removed before we train a machine learning model. First, let’s get a sense of how many missing values are in each column \n","\n","While we always want to be careful about removing information, if a column has a high percentage of missing values, then it probably will not be useful to our model. The threshold for removing columns should depend on the problem"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPjNAfGjidCt"},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"B-WM0u3HidCt"},"outputs":[],"source":["def missing_values_table(df):\n","        mis_val = df.isnull().sum()\n","        mis_val_percent = 100 * df.isnull().sum() / len(df)\n","        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n","        mis_val_table_ren_columns = mis_val_table.rename(\n","        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n","        mis_val_table_ren_columns = mis_val_table_ren_columns[\n","            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n","        '% of Total Values', ascending=False).round(1)\n","        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n","            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n","              \" columns that have missing values.\")\n","        return mis_val_table_ren_columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvUxg73kidCt"},"outputs":[],"source":["missing_values_table(df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KRN1desQidCu"},"outputs":[],"source":["# Get the columns with > 50% missing\n","missing_df = missing_values_table(df);\n","missing_columns = list(missing_df[missing_df['% of Total Values'] > 50].index)\n","print('We will remove %d columns.' % len(missing_columns))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"pX_FL3YCidCu"},"outputs":[],"source":["# Drop the columns\n","df = df.drop(list(missing_columns))"]},{"cell_type":"markdown","metadata":{"id":"oXEdvkHGidCu"},"source":["### 2.3.2. Outliers <a class=\"anchor\" id=\"Outliers\"></a>\n","At this point, we may also want to remove outliers. These can be due to typos in data entry, mistakes in units, or they could be legitimate but extreme values. For this project, we will remove anomalies based on the definition of extreme outliers:\n","\n","https://www.itl.nist.gov/div898/handbook/prc/section1/prc16.htm\n","\n","- Below the first quartile − 3 ∗ interquartile range\n","- Above the third quartile + 3 ∗ interquartile range"]},{"cell_type":"markdown","metadata":{"id":"TXPnBgOTidCu"},"source":["## 2.4 Initial Data Exploration  <a class=\"anchor\" id=\"Exploredata\"></a>\n","During this stage you'll address data mining questions using querying, data visualization and reporting techniques. These may include:\n","\n","- **Distribution** of key attributes (for example, the target attribute of a prediction task)\n","- **Relationships** between pairs or small numbers of attributes\n","- Results of **simple aggregations**\n","- **Properties** of significant sub-populations\n","- **Simple** statistical analyses\n","\n","These analyses may directly address your data mining goals. They may also contribute to or refine the data description and quality reports, and feed into the transformation and other data preparation steps needed for further analysis. \n","\n","- **Data exploration report** - Describe results of your data exploration, including first findings or initial hypothesis and their impact on the remainder of the project. If appropriate you could include graphs and plots here to indicate data characteristics that suggest further examination of interesting data subsets."]},{"cell_type":"markdown","metadata":{"id":"lHW_f9_gidCu"},"source":["### 2.4.1 Distributions  <a class=\"anchor\" id=\"Distributions\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Vwqhb6xDidCv"},"outputs":[],"source":["def count_values_table(df):\n","        count_val = df.value_counts()\n","        count_val_percent = 100 * df.value_counts() / len(df)\n","        count_val_table = pd.concat([count_val, count_val_percent.round(1)], axis=1)\n","        count_val_table_ren_columns = count_val_table.rename(\n","        columns = {0 : 'Count Values', 1 : '% of Total Values'})\n","        return count_val_table_ren_columns"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"WKuUcWV0idCv"},"outputs":[],"source":["# Histogram\n","def hist_chart(df, col):\n","        plt.style.use('fivethirtyeight')\n","        plt.hist(df[col].dropna(), edgecolor = 'k');\n","        plt.xlabel(col); plt.ylabel('Number of Entries'); \n","        plt.title('Distribution of '+col);"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBDa5xCnidCv"},"outputs":[],"source":["col = 'account_risk_band'\n","# Histogram & Results\n","hist_chart(df, col)\n","count_values_table(df.account_risk_band)"]},{"cell_type":"markdown","metadata":{"id":"4tFh2uxMidCv"},"source":["### 2.4.2 Correlations  <a class=\"anchor\" id=\"Correlations\"></a>\n","Can we derive any correlation from this data-set. Pairplot chart gives us correlations, distributions and regression path\n","Correlogram are awesome for exploratory analysis. It allows to quickly observe the relationship between every variable of your matrix. \n","It is easy to do it with seaborn: just call the pairplot function\n","\n","Pairplot Documentation cab be found here: https://seaborn.pydata.org/generated/seaborn.pairplot.html"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Vahv2gFuidCv"},"outputs":[],"source":["#Seaborn allows to make a correlogram or correlation matrix really easily. \n","#sns.pairplot(df.dropna().drop(['x'], axis=1), hue='y', kind ='reg')\n","\n","#plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x4eDusJDidCv"},"outputs":[],"source":["#df_agg = df.drop(['x'], axis=1).groupby(['y']).sum()\n","df_agg = df.groupby(['y']).sum()"]},{"cell_type":"markdown","metadata":{"id":"Jef8WDpSidCv"},"source":["### Differencing\n","Differencing\n","Specifically, a new series is constructed where the value at the current time step is calculated \n","as the difference between the original observation and the observation at the previous time step.\n","value(t) = observation(t) - observation(t-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"cMqvFqfHidCw"},"outputs":[],"source":["df_dif_agg = df_agg"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"7f4_Pn3eidCw"},"outputs":[],"source":["#Differencing\n","#Specifically, a new series is constructed where the value at the current time step is calculated \n","#as the difference between the original observation and the observation at the previous time step.\n","#value(t) = observation(t) - observation(t-1)\n","df_dif = df_dif_agg.diff()"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"i4FpzEh9idCw"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"JrS5x_5PidCw"},"source":["## 2.5 Data Quality Report <a class=\"anchor\" id=\"Dataqualityreport\"></a>\n","List the results of the data quality verification. If quality problems exist, suggest possible solutions. Solutions to data quality problems generally depend heavily on both data and business knowledge."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lZBvx1x9idCw"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["#Clase 3 🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁"],"metadata":{"id":"XGOodtb-jYDn"}},{"cell_type":"markdown","metadata":{"id":"RcF8g6NZidCw"},"source":["# 3. Stage Three - Data Preparation <a class=\"anchor\" id=\"Datapreperation\"></a>\n","This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table."]},{"cell_type":"markdown","metadata":{"id":"7H5PSrSlidCw"},"source":["## 3.1 Select Your Data <a class=\"anchor\" id=\"Selectyourdata\"></a>\n","This is the stage of the project where you decide on the data that you're going to use for analysis. The criteria you might use to make this decision include the relevance of the data to your data mining goals, the quality of the data, and also technical constraints such as limits on data volume or data types. Note that data selection covers selection of attributes (columns) as well as selection of records (rows) in a table.\n","\n","Rationale for inclusion/exclusion - List the data to be included/excluded and the reasons for these decisions."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Zp0lvaeYidCw"},"outputs":[],"source":["X_train_regr = df.drop(['date_maint', 'account_open_date'], axis = 1)\n","X_train = df.drop(['target', 'date_maint', 'account_open_date'], axis = 1)\n","X_test = test.drop(['date_maint', 'account_open_date'], axis = 1)"]},{"cell_type":"markdown","metadata":{"id":"EmWIvfysidCx"},"source":["## 3.2 Clean The Data <a class=\"anchor\" id=\"Cleansethedata\"></a>\n","This task involves raise the data quality to the level required by the analysis techniques that you've selected. This may involve selecting clean subsets of the data, the insertion of suitable defaults, or more ambitious techniques such as the estimation of missing data by modelling."]},{"cell_type":"markdown","metadata":{"id":"DduG11QnidCx"},"source":["### 3.2.1 Label Encoding <a class=\"anchor\" id=\"labelEncoding\"></a>\n","Label Encoding to turn Categorical values to Integers\n","\n","An approach to encoding categorical values is to use a technique called label encoding. Label encoding is simply converting each value in a column to a number. For example, the body_style column contains 5 different values. We could choose to encode it like this:\n","\n","convertible -> 0\n","hardtop -> 1\n","hatchback -> 2\n","sedan -> 3\n","wagon -> 4\n","http://pbpython.com/categorical-encoding.html"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"9flUpNU6idCx"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n","for col in CAT_COLS:\n","        encoder = LabelEncoder()\n","        X_train[col] = encoder.fit_transform(X_train[col].astype(str))\n","        X_test[col] = encoder.transform(X_test[col].astype(str))"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"NbIU35DpidCx"},"outputs":[],"source":["df[\"column\"] = df[\"column\"].astype('category')\n","df.dtypes"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"ZvAaFX9xidCx"},"outputs":[],"source":["df[\"column\"] = df[\"column\"].cat.codes\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"Zx3GCkTXidCx"},"source":["### 3.2.2 Drop Unnecessary Columns <a class=\"anchor\" id=\"DropCols\"></a>\n","Sometimes we may not need certain columns. We can drop to keep only relevent data"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"K02KpMwFidCx"},"outputs":[],"source":["del_col_list = ['col1', 'col2']\n","\n","df = df.drop(del_col_list, axis=1)\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"fwuBVIk-idCy"},"source":["### 3.2.3 Altering Data Types <a class=\"anchor\" id=\"AlteringDatatypes\"></a>\n","Sometimes we may need to alter data types. Including to/from object datatypes"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"mBAVlTlNidCy"},"outputs":[],"source":["#df['date'] = pd.to_datetime(df['date'])"]},{"cell_type":"markdown","metadata":{"id":"aDHHCUHsidC2"},"source":["### 3.2.4 Dealing With Zeros <a class=\"anchor\" id=\"DealingZeros\"></a>\n","Replacing all the zeros from cols. **Note** You may not want to do this - add / remove as required"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"qMm5YQ5xidC2"},"outputs":[],"source":["#cols = ['col1', 'col2']\n","#df[cols] = df[cols].replace(0, np.nan)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"xGhmSX18idC2"},"outputs":[],"source":["# dropping all the rows with na in the columns mentioned above in the list.\n","\n","# df.dropna(subset=cols, inplace=True)\n"]},{"cell_type":"markdown","metadata":{"id":"fxwt2Xk9idC3"},"source":["### 3.2.5 Dealing With Duplicates <a class=\"anchor\" id=\"DealingDuplicates\"></a>\n","Remove duplicate rows. **Note** You may not want to do this - add / remove as required"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Q5Gm5dLCidC3"},"outputs":[],"source":["#df = df.drop_duplicates(keep='first')"]},{"cell_type":"markdown","metadata":{"id":"316eDze8idC3"},"source":["## 3.3 Construct Required Data   <a class=\"anchor\" id=\"Constructrequireddata\"></a>\n","This task includes constructive data preparation operations such as the production of derived attributes or entire new records, or transformed values for existing attributes.\n","\n","**Derived attributes** - These are new attributes that are constructed from one or more existing attributes in the same record, for example you might use the variables of length and width to calculate a new variable of area.\n","\n","**Generated records** - Here you describe the creation of any completely new records. For example you might need to create records for customers who made no purchase during the past year. There was no reason to have such records in the raw data, but for modelling purposes it might make sense to explicitly represent the fact that particular customers made zero purchases.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"1Ro71IUPidC3"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"YOmxPiELidC3"},"source":["## 3.4 Integrate Data  <a class=\"anchor\" id=\"Integratedata\"></a>\n","These are methods whereby information is combined from multiple databases, tables or records to create new records or values.\n","\n","**Merged data** - Merging tables refers to joining together two or more tables that have different information about the same objects. For example a retail chain might have one table with information about each store’s general characteristics (e.g., floor space, type of mall), another table with summarised sales data (e.g., profit, percent change in sales from previous year), and another with information about the demographics of the surrounding area. Each of these tables contains one record for each store. These tables can be merged together into a new table with one record for each store, combining fields from the source tables.\n","\n","**Aggregations** - Aggregations refers to operations in which new values are computed by summarising information from multiple records and/or tables. For example, converting a table of customer purchases where there is one record for each purchase into a new table where there is one record for each customer, with fields such as number of purchases, average purchase amount, percent of orders charged to credit card, percent of items under promotion etc.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"NwDmGW-4idC3"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"GoRU1e3LidC3"},"source":["### Construct Our Primary Data Set\n","Join data "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5MXCThc0idC3"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["#Clase 4 🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁🏁"],"metadata":{"id":"N65hodhjloyo"}},{"cell_type":"markdown","metadata":{"id":"-r4ZnfklidC3"},"source":["# 4. Stage Four - Exploratory Data Analysis <a class=\"anchor\" id=\"EDA\"></a>"]},{"cell_type":"markdown","metadata":{"id":"piN7yibEidC3"},"source":["# 5. Stage Four - Modelling <a class=\"anchor\" id=\"Modelling\"></a>\n","As the first step in modelling, you'll select the actual modelling technique that you'll be using. Although you may have already selected a tool during the business understanding phase, at this stage you'll be selecting the specific modelling technique e.g. decision-tree building with C5.0, or neural network generation with back propagation. If multiple techniques are applied, perform this task separately for each technique.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8Iin70xfidC4"},"source":["## 5.1. Modelling technique <a class=\"anchor\" id=\"ModellingTechnique\"></a>\n","Document the actual modelling technique that is to be used.\n","\n","Import Models below:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"vJKNCqDSidC4"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"OzzPJKC2idC4"},"source":["## 5.2. Modelling assumptions <a class=\"anchor\" id=\"ModellingAssumptions\"></a>\n","Many modelling techniques make specific assumptions about the data, for example that all attributes have uniform distributions, no missing values allowed, class attribute must be symbolic etc. Record any assumptions made.\n","\n","- \n","- \n"]},{"cell_type":"markdown","metadata":{"id":"r0rr7UNWidC4"},"source":["## 5.3. Build Model <a class=\"anchor\" id=\"BuildModel\"></a>\n","Run the modelling tool on the prepared dataset to create one or more models.\n","\n","**Parameter settings** - With any modelling tool there are often a large number of parameters that can be adjusted. List the parameters and their chosen values, along with the rationale for the choice of parameter settings.\n","\n","**Models** - These are the actual models produced by the modelling tool, not a report on the models.\n","\n","**Model descriptions** - Describe the resulting models, report on the interpretation of the models and document any difficulties encountered with their meanings."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"AXubQZ7cidC4"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"gPyl9umbidC4"},"source":["## 5.4. Assess Model <a class=\"anchor\" id=\"AssessModel\"></a>\n","Interpret the models according to your domain knowledge, your data mining success criteria and your desired test design. Judge the success of the application of modelling and discovery techniques technically, then contact business analysts and domain experts later in order to discuss the data mining results in the business context. This task only considers models, whereas the evaluation phase also takes into account all other results that were produced in the course of the project.\n","\n","At this stage you should rank the models and assess them according to the evaluation criteria. You should take the business objectives and business success criteria into account as far as you can here. In most data mining projects a single technique is applied more than once and data mining results are generated with several different techniques. \n","\n","**Model assessment** - Summarise the results of this task, list the qualities of your generated models (e.g.in terms of accuracy) and rank their quality in relation to each other.\n","\n","**Revised parameter settings** - According to the model assessment, revise parameter settings and tune them for the next modelling run. Iterate model building and assessment until you strongly believe that you have found the best model(s). Document all such revisions and assessments."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Zl6RIN_OidC4"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"CRISP-DM Analysis Template.ipynb","provenance":[],"collapsed_sections":["wzNzfXODidCt","oXEdvkHGidCu","lHW_f9_gidCu","4tFh2uxMidCv","Jef8WDpSidCv","DduG11QnidCx","Zx3GCkTXidCx","fwuBVIk-idCy","aDHHCUHsidC2","fxwt2Xk9idC3"]}},"nbformat":4,"nbformat_minor":0}